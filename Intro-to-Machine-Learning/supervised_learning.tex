\documentclass[a4paper,12pt]{article}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{float}
\usepackage{array}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tikz}
\usepackage{amsmath}    
\usepackage{booktabs}
\usepackage{tabularx}


\title{Supervised Learning}
\author{Christian Darvin}
\date{\today}

\begin{document}

\maketitle
\section*{Core Features}
\begin{enumerate}
    \item Data
    \item Model
    \item Training
    \item Evaluating
    \item Inference
\end{enumerate}

\section*{Data}
Datasets consist of individual entries that contain \textbf{features} $(X)$ and a \textbf{label} $(y)$. It similar to a single row in a spreadsheet. \newline
\textbf{Features}: values that a supervised model uses to predict the label. \newline
\textbf{Label}: value that we want to predict. \newline

\begin{center}
\begin{tabularx}{\textwidth}{@{}lX@{}}
\toprule
\textbf{Feature} & \textbf{Value} \\
\midrule
Operating System & Windows 11 \\
CPU & Intel Core i7-12700H \\
RAM & 16GB DDR4 \\
GPU & NVIDIA RTX 3060 \\
Brand & ASUS \\
Screen Size and Type & 15.6" Full HD IPS \\
\textbf{Price (\$)} & \textbf{1,299} \\
\bottomrule
\end{tabularx}
\end{center}

\noindent Good datasets are both large (high in quantity) and highly diverse (covering a wide range of categories). Datasets with more features doesn't always produce better predictions because some features might not have significance to the label.

\section*{Model}
A mathematical model that defines the relationship between input feature patterns and output labels.

\section*{Training}
Training a model requires a dataset consisting of input features and their corresponding labels. The objective is to find the best solution for predicting the labels from the features. How do we determine if it's a good solution? By comparing the model's predictions to the actual labels. The difference between them is used to compute the \textbf{loss}. This loss guides how the model updates its internal parameters to improve its predictions over time. \newline

\noindent Prediction $\rightarrow$ Loss Calculation $\rightarrow$ Parameter Update

\noindent Throughout training, we can experiment with different parameters and input features to improve the model's predictions. Feature selection allows us to choose which inputs the model uses.

\section*{Evaluating}
In this stage, we evaluate how well the model performs. We compare the model's prediction to the label's true values. \newline

\noindent Prediction $\rightarrow$ Loss Calculation ($\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$)

\section*{Inference}
Once the model has achieved good performance, it can be used to make predictions on unlabeled data.

\section*{References}
\begin{itemize}
    \item \href{https://developers.google.com/machine-learning/intro-to-ml/supervised}{Supervised Learning - Google Developers}
    \item \href{https://developers.google.com/machine-learning/glossary}{Machine Learning Glossary - Google Developers}
\end{itemize}


\end{document}